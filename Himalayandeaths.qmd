---
title: "Himalayan_Expedition_Deaths"
author: Jayjit Das
code-fold: true
code-tools: true
format:
  html:
    toc: true
    warning: false
---

This [dataset](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-09-22/readme.md) allows us to delve into feature engineering procedures, such as subsampling to address class imbalance (given the significantly higher number of survivors compared to fatalities) and imputing missing data, particularly for expedition members with incomplete information, such as age.

## Goal: To predict the probability of an Himalayan expedition member surviving or dying

```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
library(plotly)
library(knitr)
```

## Exploratory data analysis

Loading and exploring the dataset

```{r}
members <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv")
skim(members)
```

How has the rate of expedition success and member death changed over time?

```{r}
plot1 <- members %>%
  group_by(year = 10 * (year %/% 10)) %>%
  summarise(
    died = mean(died),
    success = mean(success)
  ) %>%
  pivot_longer(died:success, names_to = "outcome", values_to = "percent") %>%
  ggplot(aes(year, percent, color = outcome)) +
  geom_line(alpha = 0.7, size = 1.5) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "year", y = "% of expedition members", color = NULL)

plotly::ggplotly(plot1) 

```

Is there a relationship between the expedition member’s age and success of the expedition or death? We can use the same code but just switch out year for age.

```{r}
plot2 <- members %>%
  group_by(age = 10 * (age %/% 10)) %>%
  summarise(
    died = mean(died),
    success = mean(success)
  ) %>%
  pivot_longer(died:success, names_to = "outcome", values_to = "percent") %>%
  ggplot(aes(age, percent, color = outcome)) +
  geom_line(alpha = 0.7, size = 1.5) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "age", y = "% of expedition members", color = NULL)

plotly::ggplotly(plot2) 
```

Are people more likely to die on unsuccessful expeditions?

```{r}
members %>%
  count(success, died) %>%
  group_by(success) %>%
  mutate(percent = scales::percent(n / sum(n))) %>%
  kable(
    col.names = c("Expedition success", "Died", "Number of people", "% of people"),
    align = "llrr"
  )
```

We can use a similar approach to see how different the rates of death are on different peaks in the Himalayas.

```{r}
members %>%
  filter(!is.na(peak_name)) %>%
  mutate(peak_name = fct_lump(peak_name, prop = 0.05)) %>%
  count(peak_name, died) %>%
  group_by(peak_name) %>%
  mutate(percent = scales::percent(n / sum(n))) %>%
  kable(
    col.names = c("Peak", "Died", "Number of people", "% of people"),
    align = "llrr"
  )
```

Let’s make one last exploratory plot and look at seasons. How much difference is there in survival across the four seasons?

```{r}
plot3 <- members %>%
  filter(season != "Unknown") %>%
  count(season, died) %>%
  group_by(season) %>%
  mutate(
    percent = n / sum(n),
    died = case_when(
      died ~ "Died",
      TRUE ~ "Did not die"
    )
  ) %>%
  ggplot(aes(season, percent, fill = season)) +
  geom_col(alpha = 0.8, position = "dodge", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  facet_wrap(~died, scales = "free") +
  labs(x = NULL, y = "% of expedition members")

plotly::ggplotly(plot3)
```

Let’s now create the dataset that we’ll use for modeling by filtering on some of the variables and transforming some variables to a be factors. There are still lots of NA values for age but we are going to impute those.

```{r}
members_df <- members %>%
  filter(season != "Unknown", !is.na(sex), !is.na(citizenship)) %>%
  select(peak_id, year, season, sex, age, citizenship, hired, success, died) %>%
  mutate(died = case_when(
    died ~ "died",
    TRUE ~ "survived"
  )) %>%
  mutate_if(is.character, factor) %>%
  mutate_if(is.logical, as.integer)

members_df
```

## Building a model

We'll split or spend our data to generate training and testing sets.

```{r}
set.seed(111)
members_split <- initial_split(members_df, strata = died)
members_train <- training(members_split)
members_test <- testing(members_split)

```

We use resampling to evaluate model performance. Getting those resampled sets ready.

```{r}
set.seed(111)
members_folds <- vfold_cv(members_train, strata = died)
members_folds
```

Next we build a recipe for data preprocessing.

-   First, we must tell the `recipe()` what our model is going to be (using a formula here) and what our training data is.

-   Next, we impute the missing values for `age` using the median age in the training data set. There are more complex [steps available for imputation](https://recipes.tidymodels.org/reference/index.html#section-step-functions-imputation), but we’ll stick with a straightforward option here.

-   Next, we use `step_other()` to collapse categorical levels for peak and citizenship. Before this step, there were hundreds of values in each variable.

-   After this, we can create indicator variables for the non-numeric, categorical values, except for the outcome `died` which we need to keep as a factor.

-   Finally, there are many more people who survived their expedition than who died (thankfully) so [we will use `step_smote()` to balance the classes](https://themis.tidymodels.org/reference/step_smote.html).

The object `members_rec` is a recipe that has **not** been trained on data yet (for example, which categorical levels should be collapsed has not been calculated).

```{r}
library(themis)

members_rec <- recipe(died ~ ., data = members_train) %>%
  step_impute_median(age) %>%
  step_other(peak_id, citizenship) %>%
  step_dummy(all_nominal(), -died) %>%
  step_smote(died)

members_rec
```

Comparing two different models: a [logistic regression](#0) model and a [random forest model](#0).

```{r}
glm_spec <- logistic_reg() %>%
  set_engine("glm")

glm_spec
```

```{r}
rf_spec <- rand_forest(trees = 1000) %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_spec
```

Putting together tidymodels workflow()

```{r}
members_wf <- workflow() %>%
  add_recipe(members_rec)

members_wf
```

Now we can add a model, and the fit to each of the resamples. First, we can fit the logistic regression model. Setting a non-default metric set so we can add sensitivity and specificity.

```{r}

members_metrics <- metric_set(roc_auc, accuracy, sensitivity, specificity)

glm_rs <- members_wf %>%
  add_model(glm_spec) %>%
  fit_resamples(
    resamples = members_folds,
    metrics = members_metrics,
    control = control_resamples(save_pred = TRUE)
  )

glm_rs
```

Now fitting the random forest model.

```{r}
rf_rs <- members_wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = members_folds,
    metrics = members_metrics,
    control = control_resamples(save_pred = TRUE)
  ) 
```

```{r}
rf_rs
```

We have fit each of our candidate models to our resampled training set!

## Evaluating model

Let's evaluate our models.

```{r}
collect_metrics(glm_rs)

```

Well, this is middling but at least mostly consistent for the positive and negative classes. The function `collect_metrics()` extracts and formats the `.metrics` column from resampling results like the ones we have here.

```{r}
collect_metrics(rf_rs)
```

The accuracy is great but that sensitivity is really bad with respect to logistic regression. The random forest model has not learnt recognition of both classes properly, even with the oversampling strategy. Let’s dig deeper into how these models are doing to see this more. For example, how are they predicting the two classes?

```{r}
glm_rs %>%
  conf_mat_resampled()
```

```{r}
rf_rs %>%
  conf_mat_resampled()
```

The random forest model is quite bad at identifying which expedition members died, while the logistic regression model does about the same for both classes.

### Visualizing the ROC curve

```{r}
plot5 <- glm_rs %>%
  collect_predictions() %>%
  group_by(id) %>%
  roc_curve(died, .pred_died) %>%
  ggplot(aes(1 - specificity, sensitivity, color = id)) +
  geom_abline(lty = 2, color = "gray9 ", size = 1.5) +
  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
  coord_equal()

plotly::ggplotly(plot5)

```

It is finally time for us to return to the testing set. Notice that we have not used the testing set yet during this whole analysis; to compare and assess models we used resamples of the training set. Let’s *fit* one more time to the training data and *evaluate* on the testing data using the function `last_fit()`.

```{r}
members_final <- members_wf %>%
  add_model(glm_spec) %>%
  last_fit(members_split)

members_final
```

The metrics and predictions here are on the *testing* data.

```{r}
collect_metrics(members_final)
```

### Confusion matrix

```{r}
collect_predictions(members_final) %>%
  conf_mat(died, .pred_class)
```

### Coefficients of predictors

The coefficients (which we can get out using `tidy()`) have been estimated using the *training* data. If we use `exponentiate = TRUE`, we have odds ratios.

```{r}
members_final %>%
  pull(.workflow) %>%
  pluck(1) %>%
  tidy(exponentiate = TRUE) %>%
  arrange(estimate) %>%
  kable(digits = 3)
```

### Visualizing the result

```{r}
plot4 <- members_final %>%
  pull(.workflow) %>%
  pluck(1) %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(estimate, fct_reorder(term, estimate))) +
  geom_vline(xintercept = 0, color = "gray7", lty = 2, size = 1.2) +
  geom_errorbar(aes(
    xmin = estimate - std.error,
    xmax = estimate + std.error
  ),
  width = .2, color = "gray7", alpha = 0.7
  ) +
  geom_point(size = 2, color = "darkcyan") +
  labs(y = NULL, x = "Coefficent from logistic regression")

plotly::ggplotly(plot4)
```

-   The features with coefficients on the positive side (like climbing in summer, being on a successful expedition, or being from the UK or US) are associated with surviving.

-   The features with coefficients on the negative side (like climbing specific peaks including Everest, being one of the hired members of a expedition, or being a man) are associated with dying.

We need to consider the interpretation of model coefficients in the context of our model's moderate predictive accuracy. The model may not capture all the factors influencing expedition survival. Additionally, the results indicate a heightened risk for native Sherpa climbers hired as expedition members in Nepal, underscoring the dangers associated with this particular demographic group during mountain expeditions.

Further readings:

1\) Himalayan Climbing Expeditions [modelling](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-09-22/readme.md).

2\) Online [platform](https://www.tidymodels.org/learn/) to learn tidymodels.

3\) Tidymodels in R [book](https://www.tmwr.org/).
